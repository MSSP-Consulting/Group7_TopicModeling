---
title: "fidelity LDA"
author: "Group 7"
date: "2022-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(topicmodels)
library(tm)
library(reshape2)
library(widyr)
library(dplyr)
library(igraph)
library(ggraph)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
```

### Data Input
```{r}
IMDB.Dataset <- read_csv("IMDB Dataset.csv", show_col_types = F)
IMDB <- tibble(IMDB.Dataset)

IMDB <- IMDB  %>%  mutate(docs = c(1:length(IMDB$review)))

data(stop_words)
stop_words <- rbind(stop_words,c("br",NA),c("movie",NA),c("movies",NA),c("film",NA))
```

### tf-idf
```{r}
book_words <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word, sort = TRUE)

text_df <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)




total_words <- book_words %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

freq_by_rank <- book_words %>% 
  group_by(docs) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

book_tf_idf <- book_words %>%
  bind_tf_idf(word, docs, n)


```

### Words Network
```{r}
set.seed(1234)

review_words_pair <- book_words %>% 
  pairwise_count(word, docs, sort = TRUE, upper = FALSE)
review_words_pair %>%
  filter(n >= 2200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

```{r}
set.seed(2000)

review_words_pair <- book_words %>% 
  pairwise_count(word, docs, sort = TRUE, upper = FALSE)
review_words_pair %>%
  filter(n >= 3000) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "red") +
  geom_node_point(size = 4) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.3, "lines")) +
  theme_void()
```

### Word Cloud
```{r}
set.seed(1234)
wordcloud(words = text_df$word, freq = text_df$n, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```



### LDA
```{r}
imdb_dtm <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word) %>%
  cast_dtm(docs, word, n)


review_lda <- LDA(imdb_dtm, k = 12, control = list(seed = 1234))

tidy_lda <- tidy(review_lda, matrix = "beta")
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
<<<<<<< HEAD
=======


beta_wide <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
beta_wide

>>>>>>> 168909316c6637c18caa95e0ec097e4f000132d9


beta_wide <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
beta_wide


```
