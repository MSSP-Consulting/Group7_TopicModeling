---
title: "Topic Modeling Report"
author: "Group 7"
date: "2022-11-12"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(topicmodels)
library(tm)
library(reshape2)
library(widyr)
library(dplyr)
library(igraph)
library(ggraph)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(textstem)
```

  We use IMDB reviews as our data and do the following anylsis. First, we set up three stop words, movie, movies and film, since these three words appear most frequently and do not make much sense in movie reviews. Then, we use $unnest_tokens()$ function to both break the text into individual tokens and transform it to a tidy data structure.

```{r}
#data input.
IMDB.Dataset <- read_csv("IMDB Dataset.csv", show_col_types = F)
IMDB <- tibble(IMDB.Dataset)

IMDB <- IMDB  %>%  mutate(docs = c(1:length(IMDB$review)))

#set stop words
data(stop_words)
stop_words <- rbind(stop_words,c("br",NA),c("movie",NA),c("movies",NA),c("film",NA))
```
  Afterwards, we look at the terms' inverse document frequency to measure if a word is important in our analysis. As a result, considering both the number of occurrences and term frequency(n/total), we have top 5 terms, 'trivialboring', 'mad', 'stop.oz', 'bad', and 'bought'. Three of them are negative, so that we may conclude that there are more negative review than the positive. In order to prove it, we further draw some plots.

```{r}
#tf-idf
book_words <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word, sort = TRUE)%>% 
  mutate(word = word%>% 
           lemmatize_words())

text_df <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)%>% mutate(word = word%>% lemmatize_words())

total_words <- book_words %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

freq_by_rank <- book_words %>% 
  group_by(docs) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

book_tf_idf <- book_words %>%
  bind_tf_idf(word, docs, n)

```
```{r}
fre<-freq_by_rank%>%arrange(desc(`term frequency`))
```


Here, we draw a plot showing how words correlate to each other. We filter the words by saying that the total number of occurrence is larger than 2200 and get 26 words, which is centered around 'time', 'story', 'bad' and 'people'.




```{r}
#Words Network
set.seed(1234)

review_words_pair <- book_words %>% 
  pairwise_count(word, docs, sort = TRUE, upper = FALSE)
review_words_pair %>%
  filter(n >= 2200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```






Another plot shows how often these words show up in the review. The larger the word, the more often it appears. Obviously, 'time' and 'story' are the two most frequent occurrences, followed by 'bad' and 'people', and then 'love', 'films', etc..

```{r}
#Word Cloud
set.seed(1234)
wordcloud(words = text_df$word, freq = text_df$n, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```


Finally, we use LDA model to find 12 topics that are extracted from the review. The most common words in topic 1 include "bad", "watch","10", and "original", which suggests it may represent literary film. Those most common in topic 2 include "makes", "acting", "funny", and "real", suggesting that this topic represents comedy or action movie. When analyze these topic, we tend to ignore the words that appears in every topic including "story", "character", and "scene".
```{r}
#LDA
imdb_dtm <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word) %>%
  cast_dtm(docs, word, n)


review_lda <- LDA(imdb_dtm, k = 12, control = list(seed = 1234))

tidy_lda <- tidy(review_lda, matrix = "beta")
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)
```
```{r}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4.6, scales = "free")+
  theme(axis.text.x = element_text(size=5),axis.text.y = element_text(size=5))
```

